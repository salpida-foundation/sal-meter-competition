# Frequently Asked Questions (Extended FAQ)

This document is an **extended FAQ** for the **Sal-Meter Open Competition**,  
designed to address recurring questions with greater clarity and without ambiguity.

This FAQ is written to remain **fully aligned in content, tone, and governance structure**
with the following official canonical documents:

- **Sal-Meter System Overview v1.0**  
  https://doi.org/10.6084/m9.figshare.31275067
- **Sal-Meter Open Competition – Technical & Governance Snapshot v2.0**  
  https://doi.org/10.6084/m9.figshare.31281835
- **PI Quick Decision Pack — Sal-Meter Open Competition**  
  https://doi.org/10.6084/m9.figshare.31287268
- **CAIS Architecture & Sal-Meter Technical Snapshot v1.0**  
  https://doi.org/10.6084/m9.figshare.31287529

---

## 1. Judging Structure and the Role of SICS

### Q1. Who appoints the judges?

The **Salpida Institute of Consciousness Science (SICS)** appoints an  
**Independent Judging Panel** consisting of approximately **7–9 external experts**.

Judges are drawn from the following domains:

- Biosensors / Chemical biology  
- Aptamers and molecular interfaces  
- Signal processing / AI / statistics  
- Systems engineering  
- Research ethics and governance  

**Independence requirements**

- No direct financial interest in any participating team  
- No institutional affiliation with participating teams during the competition period  
- SICS personnel **may not serve as technical judges**

---

### Q2. What role does SICS play in the evaluation process?

SICS is **not a technical evaluation body**.

Within this competition, SICS operates solely as a  
**constitutional oversight body**.

**What SICS does**

- Maintains interpretive consistency of CAIS / Sal-Meter standards  
- Resolves disputes related to canonical document interpretation  
- Mediates rule-based conflicts between teams and judges  

**What SICS does not do**

- Score prototypes  
- Rank teams  
- Decide winners  
- Influence research direction or outcomes  

In short, SICS is the **guardian of rules**,  
not a combined **player–referee**.

---

### Q3. What are the evaluation criteria?

The Independent Judging Panel evaluates contributions using **structural and scientific criteria**, not competitive scoring logic.

| Dimension | Meaning |
|--------|--------|
| CAIS structural compliance | Alignment with CAIS and Compliance Boundary |
| Technical validity | Signal separation, reproducibility, experimental rigor |
| Exploratory contribution | Novel approaches, boundary clarification |
| Openness | Documentation quality, data disclosure, reproducibility |
| CCF alignment | Philosophical and structural coherence with CCF |

**Important**

- This is **not a winner-takes-all scorecard**.  
- The goal is to assess **whether an approach is structurally and scientifically meaningful**.  
- A lower score does not imply research failure.

---

### Q4. Could SICS favor specific teams?

The system is designed to **minimize this possibility structurally**.

1. **Separation of authority**  
   - Rule definition & interpretation: SICS  
   - Technical evaluation: Independent Panel  

2. **Open records**  
   - Core validation data and results are published in open repositories  
     (GitHub, OSF, etc.)  
   - The evaluation process itself remains auditable  

3. **Appeal mechanism**  
   - Teams may challenge outcomes  
   - Reviews address **rule compliance only**, not score adjustments  

---

## 2. Role of CRO (Independent Validation)

### Q5. Who performs final validation?

At designated stages,  
final validation is conducted by an independent  
**Contract Research Organization (CRO)**.

The CRO’s role is strictly limited to:

- Executing measurement protocols  
- Collecting validation data  
- Performing statistical verification  

The CRO **may not**:

- Reinterpret CAIS architecture  
- Propose alternative models  
- Issue certification or approval judgments  

**Interpretive authority remains fixed to the canonical documents.**

---

## 3. Team Structure and Participation

### Q6. Must Track A teams follow a fixed headcount?

No.

Recommended team structures exist, but **they are not mandatory**.

What matters is **role coverage**, not headcount:

- Molecular / interface responsibility  
- Signal processing responsibility  
- System integration responsibility  
- Experimental design & validation responsibility  

If these roles are covered, team size is flexible.

---

### Q7. Are international or mixed-nationality teams allowed?

Yes.

There are **no restrictions** based on nationality, institution, or legal form.

---

### Q8. Can students or early-stage researchers participate?

Yes.

- **Track A/B**: Experienced leadership is recommended; students may participate as contributors  
- **Track C**: Open to students, individuals, and small independent groups  

Track C is **not an entry-level track**,  
but an **exploration-first track**.

---

## 4. Clarifying Research Support and Funding

### Q9. Is this a prize-based competition?

No.

This program is **not a prize-driven contest**,  
but a **milestone-based research support program**.

- No single winner is assumed  
- Teams are not racing toward a common finish line  
- Support is structured around **initial engagement + milestone progress**

---

### Q10. How is research support provided?

Support follows these principles:

- Initial engagement support  
- Milestone-based allocation  
- Adjusted to each team’s scope, role, and experimental difficulty  
- Exception handling allowed when scientifically justified  

In other words,  
**research is not shaped to fit a fixed payout table**;  
support structures are shaped to fit research reality.

---

### Q11. Is there a penalty for failure?

No.

The program explicitly assumes that:

- Success is not guaranteed  
- Negative or null results are valid scientific outcomes  
- Boundary conditions and limitations are valuable records  

Failure is not elimination.  
It is **knowledge gained**.

---

## 5. On the Non-Competitive Nature of the Program

This program intentionally avoids:

- Single-winner structures  
- Publication-count competition  
- Speed-based rankings  
- Marketing-oriented outcomes  

Instead, it emphasizes:

- Parallel exploration  
- Independent experimentation  
- Open comparison  
- Structural validation  

The Sal-Meter Open Competition is not about  
**“who wins”**,  
but about **“what becomes knowable.”**

---

## Reference Documents

- Technical details  
  ./01_technical_details.md
- Development phases  
  ./02_phase_roadmap.md
- Team composition guide  
  ./03_team_composition.md

---

**License:** CC BY-SA 4.0  
**Last updated:** 2026-02-03
